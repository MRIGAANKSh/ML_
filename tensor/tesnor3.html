<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Liveness Detection with TensorFlow.js</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      margin: 0;
      background-color: #f0f0f0;
    }
    canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: 1;
    }
    video {
      position: absolute;
      top: 0;
      left: 0;
    }
  </style>
</head>
<body>
  <video id="webcam" autoplay playsinline muted width="640" height="480"></video>
  <canvas id="overlay" width="640" height="480"></canvas>

  <!-- TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <!-- BlazeFace model -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>

  <script>
    let model, webcam, canvas, ctx;
    let lastPosition = null;  // To store the previous face position
    let noMovementFrames = 0; // Counter to detect lack of movement (potentially fake)

    // Initialize the webcam feed and canvas for drawing
    async function setupWebcam() {
      webcam = document.getElementById('webcam');
      canvas = document.getElementById('overlay');
      ctx = canvas.getContext('2d');

      // Request webcam access
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      webcam.srcObject = stream;

      return new Promise((resolve) => {
        webcam.onloadedmetadata = () => {
          resolve(webcam);
        };
      });
    }

    // A heuristic for detecting liveness based on head movement
    function detectLiveness(prediction) {
      const [startX, startY] = prediction.topLeft;
      const [endX, endY] = prediction.bottomRight;

      const currentPosition = { x: (startX + endX) / 2, y: (startY + endY) / 2 };

      if (lastPosition) {
        const dx = Math.abs(currentPosition.x - lastPosition.x);
        const dy = Math.abs(currentPosition.y - lastPosition.y);

        // Consider it real if there's sufficient movement
        if (dx > 10 || dy > 10) {
          noMovementFrames = 0;  // Reset if movement detected
          return true;            // Face is real
        } else {
          noMovementFrames++;     // Increment when no significant movement
        }
      }

      lastPosition = currentPosition;

      // If face hasn't moved significantly in the last 30 frames, classify as fake
      if (noMovementFrames > 30) {
        return false; // Fake face (e.g., a photo or video)
      }

      return true; // Assume real until proven fake
    }

    // Load the BlazeFace model and perform face detection
    async function runFaceDetection() {
      model = await blazeface.load();

      const detectFaces = async () => {
        const predictions = await model.estimateFaces(webcam, false);

        // Clear previous drawings
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (predictions.length > 0) {
          predictions.forEach((prediction) => {
            // Get bounding box coordinates
            const [startX, startY] = prediction.topLeft;
            const [endX, endY] = prediction.bottomRight;

            // Check for liveness based on head movement
            const isReal = detectLiveness(prediction);

            // Draw bounding box with appropriate color
            ctx.beginPath();
            ctx.rect(startX, startY, endX - startX, endY - startY);
            ctx.lineWidth = 3;
            ctx.strokeStyle = isReal ? 'green' : 'red'; // Green for real, red for fake
            ctx.stroke();
          });
        }

        requestAnimationFrame(detectFaces);
      };

      detectFaces();
    }

    // Initialize everything
    setupWebcam().then(runFaceDetection);
  </script>
</body>
</html>
